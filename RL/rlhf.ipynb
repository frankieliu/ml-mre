{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22f03c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92362e40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Environment setup (simplified)\n",
    "class TextEnvironment:\n",
    "    def __init__(self):\n",
    "        self.actions = [\"Say hello\", \"Ask about weather\", \"Tell a joke\", \"Share a fact\"]\n",
    "        self.state = \"start\"\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = \"start\"\n",
    "        return self.state\n",
    "        \n",
    "    def step(self, action_idx):\n",
    "        action = self.actions[action_idx]\n",
    "        \n",
    "        # Normally you'd have a real environment response\n",
    "        if action_idx == 0:\n",
    "            response = \"Hello there!\"\n",
    "        elif action_idx == 1:\n",
    "            response = \"It's sunny today.\"\n",
    "        elif action_idx == 2:\n",
    "            response = \"Why don't scientists trust atoms? Because they make up everything!\"\n",
    "        else:\n",
    "            response = \"Did you know honey never spoils?\"\n",
    "            \n",
    "        self.state = response\n",
    "        return response, 0, False, {}  # state, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a62330",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reward Model (predicts human preference)\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1 for preference\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Human feedback simulation (in practice, you'd collect real human feedback)\n",
    "def get_human_feedback(response1, response2):\n",
    "    # Simulate human preference (in reality, you'd collect this from actual humans)\n",
    "    preferences = {\n",
    "        \"Hello there!\": 0.8,\n",
    "        \"It's sunny today.\": 0.6,\n",
    "        \"Why don't scientists trust atoms? Because they make up everything!\": 0.9,\n",
    "        \"Did you know honey never spoils?\": 0.7\n",
    "    }\n",
    "    \n",
    "    pref1 = preferences.get(response1, 0.5)\n",
    "    pref2 = preferences.get(response2, 0.5)\n",
    "    \n",
    "    if pref1 > pref2:\n",
    "        return 1  # Prefer first response\n",
    "    elif pref2 > pref1:\n",
    "        return 2  # Prefer second response\n",
    "    else:\n",
    "        return 0  # No preference\n",
    "\n",
    "# Text embedding (simplified)\n",
    "def embed_text(text):\n",
    "    # In practice, use a proper text embedding like BERT\n",
    "    return torch.randn(128)  # Random embedding for demonstration\n",
    "\n",
    "def train_rlhf():\n",
    "    # Initialize components\n",
    "    env = TextEnvironment()\n",
    "    policy = PolicyNetwork(128, len(env.actions))\n",
    "    reward_model = RewardModel(128)\n",
    "    optimizer_policy = optim.Adam(policy.parameters(), lr=0.001)\n",
    "    optimizer_reward = optim.Adam(reward_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Experience buffer for RLHF\n",
    "    buffer = deque(maxlen=1000)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(100):\n",
    "        state = env.reset()\n",
    "        state_embed = embed_text(state)\n",
    "        \n",
    "        # Generate two responses for comparison\n",
    "        with torch.no_grad():\n",
    "            logits1 = policy(state_embed)\n",
    "            prob1 = torch.softmax(logits1, dim=-1)\n",
    "            action1 = torch.multinomial(prob1, 1).item()\n",
    "            \n",
    "            logits2 = policy(state_embed)\n",
    "            prob2 = torch.softmax(logits2, dim=-1)\n",
    "            action2 = torch.multinomial(prob2, 1).item()\n",
    "        \n",
    "        response1, _, _, _ = env.step(action1)\n",
    "        response2, _, _, _ = env.step(action2)\n",
    "        \n",
    "        # Get human feedback\n",
    "        preference = get_human_feedback(response1, response2)\n",
    "        \n",
    "        # Store in buffer\n",
    "        if preference != 0:\n",
    "            buffer.append((state_embed, action1, action2, preference))\n",
    "        \n",
    "        # Train reward model if we have enough data\n",
    "        if len(buffer) >= 32:\n",
    "            batch = random.sample(buffer, 32)\n",
    "            \n",
    "            # Prepare data\n",
    "            states = torch.stack([x[0] for x in batch])\n",
    "            actions1 = torch.tensor([x[1] for x in batch])\n",
    "            actions2 = torch.tensor([x[2] for x in batch])\n",
    "            preferences = torch.tensor([x[3] for x in batch])\n",
    "            \n",
    "            # Train reward model\n",
    "            optimizer_reward.zero_grad()\n",
    "            \n",
    "            # Get rewards for each action\n",
    "            rewards1 = reward_model(states)\n",
    "            rewards2 = reward_model(states)\n",
    "            \n",
    "            # Calculate loss based on preferences\n",
    "            loss = 0\n",
    "            for i in range(len(batch)):\n",
    "                if preferences[i] == 1:\n",
    "                    loss += -torch.log(rewards1[i] / (rewards1[i] + rewards2[i]))\n",
    "                elif preferences[i] == 2:\n",
    "                    loss += -torch.log(rewards2[i] / (rewards1[i] + rewards2[i]))\n",
    "            \n",
    "            loss = loss / len(batch)\n",
    "            loss.backward()\n",
    "            optimizer_reward.step()\n",
    "            \n",
    "            # Train policy using rewards\n",
    "            optimizer_policy.zero_grad()\n",
    "            \n",
    "            # Get current policy's actions\n",
    "            logits = policy(states)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get rewards for these actions\n",
    "            with torch.no_grad():\n",
    "                rewards = reward_model(states)\n",
    "            \n",
    "            # Reinforce actions that lead to higher rewards\n",
    "            selected_probs = probs.gather(1, actions1.unsqueeze(1))\n",
    "            policy_loss = -torch.log(selected_probs) * rewards1.detach()\n",
    "            policy_loss = policy_loss.mean()\n",
    "            \n",
    "            policy_loss.backward()\n",
    "            optimizer_policy.step()\n",
    "            \n",
    "            print(f\"Episode {episode}, Reward Loss: {loss.item():.4f}, Policy Loss: {policy_loss.item():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_rlhf()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
