{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdd77b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# ---- Setup Environment ----\n",
    "env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "\n",
    "# ---- Policy Network ----\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "# ---- Reward Model ----\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, traj_len, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(traj_len * obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, traj_flat):\n",
    "        return self.net(traj_flat)\n",
    "\n",
    "# ---- Collect Trajectory ----\n",
    "def collect_trajectory(env, policy, traj_len=50):\n",
    "    obs = env.reset()[0]\n",
    "    traj = []\n",
    "    for _ in range(traj_len):\n",
    "        obs_tensor = torch.tensor([obs], dtype=torch.float32)\n",
    "        probs = policy(obs_tensor)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        next_obs, _, terminated, truncated, _ = env.step(action)\n",
    "        traj.append(obs)\n",
    "        obs = next_obs\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    # pad trajectory\n",
    "    while len(traj) < traj_len:\n",
    "        traj.append(np.zeros_like(traj[0]))\n",
    "    return torch.tensor(traj, dtype=torch.float32)\n",
    "\n",
    "# ---- Simulated Human Feedback ----\n",
    "def synthetic_preference(t1, t2):\n",
    "    def stability_score(traj):\n",
    "        return -torch.mean(torch.abs(traj[:, 2]))  # pole angle\n",
    "    score1 = stability_score(t1)\n",
    "    score2 = stability_score(t2)\n",
    "    return 0 if score1 > score2 else 1\n",
    "\n",
    "# ---- Train Reward Model ----\n",
    "def train_reward_model(reward_model, pairs, labels, epochs=5):\n",
    "    opt = optim.Adam(reward_model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    for _ in range(epochs):\n",
    "        for (t1, t2), label in zip(pairs, labels):\n",
    "            r1 = reward_model(t1.view(1, -1))\n",
    "            r2 = reward_model(t2.view(1, -1))\n",
    "            logits = r1 - r2\n",
    "            target = torch.tensor([[1.0 if label == 0 else 0.0]])\n",
    "            loss = loss_fn(logits, target)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "# ---- Policy Update ----\n",
    "def update_policy(policy, reward_model, env, steps=20):\n",
    "    opt = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    for _ in range(steps):\n",
    "        traj = collect_trajectory(env, policy)\n",
    "        r = reward_model(traj.view(1, -1))\n",
    "        loss = -r\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "# ---- Evaluation ----\n",
    "def evaluate_policy(policy, env, episodes=5):\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()[0]\n",
    "        total = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor([obs], dtype=torch.float32)\n",
    "            probs = policy(obs_tensor)\n",
    "            action = torch.argmax(probs).item()\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total += reward\n",
    "        total_rewards.append(total)\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "# ---- MAIN LOOP ----\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "traj_len = 50\n",
    "\n",
    "policy = PolicyNet(obs_dim, action_dim)\n",
    "reward_model = RewardModel(traj_len, obs_dim)\n",
    "\n",
    "# Pre-RLHF Evaluation\n",
    "pre_rlhf_rewards = [evaluate_policy(policy, env) for _ in range(10)]\n",
    "\n",
    "# 1. Collect data\n",
    "pairs, labels = [], []\n",
    "for _ in range(100):\n",
    "    t1 = collect_trajectory(env, policy, traj_len)\n",
    "    t2 = collect_trajectory(env, policy, traj_len)\n",
    "    label = synthetic_preference(t1, t2)\n",
    "    pairs.append((t1, t2))\n",
    "    labels.append(label)\n",
    "\n",
    "# 2. Train Reward Model\n",
    "train_reward_model(reward_model, pairs, labels)\n",
    "\n",
    "# 3. RLHF Policy Update\n",
    "rlhf_rewards = []\n",
    "for _ in range(30):\n",
    "    update_policy(policy, reward_model, env)\n",
    "    avg_reward = evaluate_policy(policy, env)\n",
    "    rlhf_rewards.append(avg_reward)\n",
    "\n",
    "# Post-RLHF Evaluation\n",
    "post_rlhf_rewards = [evaluate_policy(policy, env) for _ in range(10)]\n",
    "\n",
    "# ---- Plotting ----\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rlhf_rewards, label=\"RLHF Training Reward\")\n",
    "plt.axhline(np.mean(pre_rlhf_rewards), color='r', linestyle='--', label=\"Pre-RLHF Avg\")\n",
    "plt.axhline(np.mean(post_rlhf_rewards), color='g', linestyle='--', label=\"Post-RLHF Avg\")\n",
    "plt.xlabel(\"RLHF Iteration\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"CartPole with Simulated RLHF\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
